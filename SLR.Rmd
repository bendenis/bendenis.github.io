---
title: "Linear Regression"
author: "Ben Denis Shaffer"
output: html_document
---

#Introduction

Linear Regression is a tool for analysing data with a qunatitative response. For example energy consumption in Watts, or maximum speed of a car. One might want to understand whether enrgy consumption from a house depends on when it was built, or if the car speed depends on its weight and horspower. Age, weight and horspower in these cases are reffered to as predictos, covariates, independent variables, or factors.

Its important to understand that when using linear regression you are already assuming that you know how the response depends on the predictors. You implicitly assume that the relationship _is linear_. The model does two things for you. 

- Estimates the relationship quantitativly.
- Tests the validity of the estimates.

For example, we assume that increasing or decreasing the age of a house will increase or decrease the quantity of energy it uses. We want to know numerically how much more energy an older house would use. This aspect of the model is most useful in practice because numbers effectivly communicate infomration. It is easy to get excited about having a formula that will tell you how much electricity a house will consume, and use it to make decisions and propose policy. _However_, the most important aspect of the model is the statistical validity/significance.

The plot below illustrates the idea of statistical validity. We have three sets of points labeld as `Population`, `Random Sample`, `Biased Sample`. If you fit a model using the random sample you would be more confident in the regression because there are many more red points and they are scattered all over. If you use the blue data the regression would be wrong, and even if they were correct you would be less confident in them because there are many less observations.

```{r, message=FALSE, warning=FALSE}
library(ggplot2)
library(ggthemes)
library(plotly)

set.seed(1234)
dfp = data.frame(x = rnorm(875), y = rnorm(875), Sample = "Population")
df1 = data.frame(x = rnorm(300), y = rnorm(300), Sample = "Random Sample")
df2 = data.frame(x = rnorm(20,-1), y = rnorm(20,1), Sample = "Biased Sample")

df = rbind(dfp,df1,df2)
df$Sample = as.factor(df$Sample)
g = ggplot(df, aes(x = x, y = y)) + geom_point(aes(colour = Sample), alpha = 0.7) + scale_colour_manual(values = c("#000000","#FF0000","#0000FF")) + theme_economist()
ggplotly()

```

Understanding the data in terms of what it represents and how it came about is essential if you want to do statistics. However fitting a regression model is nothing but an optimization problem for which you don't need any statistics.

#Topics

- Simple Linear Regression
- Residuals
- Parameter Estimation
- Interpretation
- Goodness of Fit
- Prediction Accuracy
- Inferencial Statistics

### Simple Linear Regression

Smiple linear regression refers to a regression where there is just one predictor and one response. As an example you can take a measurment of body fat percentage and analyse its relatonship with chest circumference. You can look at the scatter plot of the data. You can see that it looks that greater chest circumference corresponds to greater bodyfat percentage. You assume that there is an equation 

$$\boldsymbol{Y} = \beta_0 + \beta_1 \boldsymbol{X} + \boldsymbol{\epsilon}$$
Where Y is bodyfat percentage and X is chest circumference. This is just an equation of a straight line with an additional epsilon at the very end. $\beta_0$ is the intercept and $\beta_1$ is the slope. These are unkown and are called model parameters that need to be estimated. $\boldsymbol{\epsilon}$ represents error. The reason why there must be an error term is because it is impossible to draw a straight line through the scatter plot below, such that each point is on the line.

```{r}
library(faraway)
data("fat")
data = data.frame(y = fat$brozek, x = fat$chest)
g = ggplot(data, aes(y = y, x = x)) + geom_point() + theme_economist()
ggplotly()

```

### Residuals

So you have a model and next you need to somehow estimate the unknown parameters. You want to do so in an optimal way. It makes sense to define these estimates as optimal if the errors as small as can be. If we knew the true parameters from nature then we could simply calculate each error. 

$$\boldsymbol{\epsilon}  = \boldsymbol{Y} - (\beta_0 + \beta_1 \boldsymbol{X})$$

This computation give you the error only if you know the true parameters. If you choose a parameter then what you are computing is called a residual. Note that residuals can be both positive and negative. 

$$\boldsymbol{r}  = \boldsymbol{Y} - (\tilde{\beta_0} + \tilde{\beta_1} \boldsymbol{X})$$

$\tilde{\beta_0}$ and $\tilde{\beta_1}$ are the chosen parameters that you pick.

Graphically you can imagine rubber strings conecting a straight stick to each point.
When choosing the parameters we want the tenssion in the strings to be minimal, or in other words for the distances between the points and the stick to be as samll as can be.

```{r, warning=FALSE, message=FALSE}
g = ggplot(data, aes(x = x, y = y)) + geom_point() + geom_segment(aes(x = x, y = y, xend = x, yend = mean(y), color = -(y - mean(y))^2)) + geom_segment(aes(x = min(x), y = mean(y), xend = max(x), yend = mean(y)), color = 2) + theme_economist() + guides(color = FALSE) + ggtitle("Residuals")
ggplotly()

```

For example the largest residual in the above picture is for the 216th point at about 26 units. The largest negative residual is for the 182nd poin at about negative 19. Note that in the plot the 182nd and 216th data points are not 182nd and 216th from the right since the data is not sorted in anyway. So the 182nd residual is actually the leftmost residual in the plot.

```{r}
m = lm(data$y ~ 1)
m$residuals[which.max(m$residuals)]
m$residuals[which.min(m$residuals)]
```

### Parameter Estimation

Looking at the plot above you see that points on the bottom left will pull the line down and points on the top right will pull it up. The goal is to minomize the residuals, or total distances between the points and a line. You can't measure the distances as just the vertical distances you see in the plot because some are negative and some are positive. To compute total distances we need to sum positive numbers. To achive this you square every residual. So the optimal parameters are those that minimize the squared residuals. This sum is called the Residual Sum of Squares (RSS).

$$ RSS = \sum_{i=1}^{n} r_i^2 = \sum_{i=1}^{n} [y_i - (\tilde{\beta_0} + \tilde{\beta_1} x_i)]^2 $$

The reason why we take $r_i^2$ and not for example $|r_i|$ is the Gaus-Markov theorem. This is the classical approach called Ordinary Least Squares (OLS), but any other approach is also valid and can be used and studied. If a different approach is chosen however its not refered to as Simple Linear Regression.

The solution to the minimization of RSS with respect to $(\tilde{\beta_0}, \tilde{\beta_1})$ yeilds the OLS estimated optimal parameters, denoted as $(\hat{\beta_0}, \hat{\beta_1})$.

$$ \hat{\beta_1} = \frac{\sum (x_i -\bar{x}) (y_i - \bar{y})}{\sum (x_i -\bar{x})^2}  = \frac{Cov(X,Y)}{Var(X)}$$
$$ \hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x} $$

Now given the data we have been looping at we fit the model, get the parameters and plot the line using the estimated parameters as the intercept and slope. You can see the summary and the plot below.

```{r}
library(knitr)
model = lm(y ~ x, data)
kable(summary(model)$coef)

```

Now that we use thse paramters as the slope and intercept of the line we get the following picture

```{r}

g = ggplot(data, aes(y = y, x = x)) + geom_point() + theme_economist() + geom_smooth(method = "lm", se= FALSE, col = 2) + ggtitle("Linear Model")
ggplotly()

```

### Interpretation

One of the greatest practical advanatges of linear models is their simple interpretability. The estimated slope is a chnage in Y given a unit change in X. The intercept is the value of Y when X is zero. So for example if someone tells you that an individuals chest measurment decreased by 12 units, you could predict that that individual experienced a decrease in bodyfat percentage of `-12*0.6462 = -7.7544`.The next quastion to answer is how certain you are about this prediction. For this inferencial statistics will need to be used.

### Goodness of Fit

How well a model is fit is evaluated relative to a model that doesn't use data for choosing the parameters. This model is a horizontal line going through the mean of Y. When you compute the mean of Y you don't use X at all. On the otherhand when you fit a linear model you are computing the mean of Y conditional on X.

The goal of the fitting was to minimize the squared residuals between the line and the points. So a measure of how well a model fits would be a comparison of the RSS of the two models. For the simplest model the RSS is called Total Sum of Squares (TSS). Instead of simply taking the ratio of the two, the ratio is subtracted from 1 and given the name $R^2$. The reson fot this is the interpritation.

$$ R^2 = 1 - \frac{RSS}{TSS} $$

$R^2$ is a measure of linear association. If the model is a better linear fit the RSS is lower and $R^2$ is greater. RSS is always lower than TSS which you can see from the figure with the residuals, so $R^2$ is always between 1 and 0. The interpriation is that $R^2$ is the percent of variation in Y explained by X, assuming that X and Y are linearly related.

In the current model the $R^2$ is `0.4940475` so you would say that chest circumference explains about 50% of variation in bodyfat percenatge measurment.

```{r}
summary(model)$r.squared
```

This measurment of fit can be used to compare models so a model with lower RSS and greater $R^2$ is the one with a better linear fit.

### Prediction Accuracy

A possible use of the fitted model is prediction. If you are given a new measurment of chest circumference you can use the model to predict the bodyfat percentage. This is practical because measuring chest circumference is much easier and cheaper then accuratly measuring bodyfat percentage.

So then you want to evaluate how accurate the predictions would be if you used your fitted model. The initial way of doing this is evaluating how well you predict, or how closely reconstruct the data that you had using your model. For this we use the model to predict the existing Y using only X, and call each prediction the fitted value $\boldsymbol{\hat{Y}}$ where

$$ \boldsymbol{\hat{Y}} =  \hat{\beta_0} + \hat{\beta_1} \boldsymbol{X} $$
Then you compute the mean of the squared differences in the predicted and observed values which is called the Mean Squared Error (MSE). MSE is not a measure used uniquely for regression. It's a general concept, but in the case of linrea regression it is very simple which is related to Gauss-Markov theorem mention earlier.

$$ MSE = \frac{1}{n}\sum (Y_i - \hat Y_i)^2 $$

For out model the MSE is 30.27486. This means that on average the squared error of your prediction will be about 30 units, or about 5.5% of bodyfat. As you can tell this is not very good at all.

```{r}
MSE = (1/dim(data)[1])*sum((data$y - model$fitted.values)^2)
MSE
```

In practice prediction accuracy would not be evaluated in this way. The data would be divided into a training set for fitting the model and a training set for computing MSE.

### Inferencial Statistics

The model is fit, interpreted, and even evaluated in terms of how well it fits the data. The important question is how confident you can be about these results given the data that you have. Using your data you found a relationship between bodyfat percentage and chest circumference. If you got a different set of data would you find the same relationship? Imagine you found it to be negative (not impossible).

If you make a couple of assumptions you can begin to do inferencial statistics which means that you can deturmine whether the model is valid, and evaluate how confident you can be in your predictions. The assumtion are summarised in the follwing statment:

$$ \epsilon_i \sim^{iid} N(0,\sigma^2) $$
In words this means that the error terms, the deviation from the line, are 

- Normally distributed.
- Have mean 0.
- Have the same constant variance $\sigma^2$.
- Are all mutually independent of one another.

Of course these assumptions can be a good idea, a not so good idea, or unacceptible all together. Deturmining whether these assumptions are acceptible is one of the roles of model diagnostics. The OLS algorithm for estimating parameters technically follows these assumptions about the data, but mathematically the model fitting can be done for any dataset. Sometimes your data can be transformed in acceptible ways to make it better fit these assumptions. Also, these assumptions can be relaxed and modified which leads to different algorithms that are more appropriate for different data.

These assumptions imply that the response of interest is also normally distributed:

$$ Y_i \sim^{iid} N(\beta_0 + \beta_1 X_i, \sigma^2) $$

The mean $\beta_0 + \beta_1 X_i$ is already estimtated when fitting the regression. The variance $\sigma^2$ needs to be estimated still. When you have both you can test how likely it is that given these estimated parameters the slope of the line is actually zero. In other words, you can hypothesise that the slope is zero and that there is no relationship between bodyfat percentage and chest circumference, and then use the data to either validate this hypothesis, or reject it.

This part of regression is quite mathematical and whole courses in mathematical statistics are dedicated to discussing how 'sampling distributions', 'tests', and inferences are derived depending on the assumptions made. But this doesn't mean that you can't interpret the inferences without having taken such courses. The results of the bodyfat example can be summurized in the model summary output.

```{r}

summary(model)

```

The first line is just the model call that was input. The second portion of the output shows the range and the interquartile range of the residuals from the model. This is a quick first diagnostic because you want the residulas to be symmetric around 0.

The next portion is more interesting. You have the estimates of the parameters/coefficients that you saw before, and next to them are the inferencial statistics. To the immediate right you have the `Standard Error` which can be though of as variation of the actual coefficient. In short, you want the SE to be small relative to the coefficient. The t-value and p-value are derived from the SE and the coefficient. You want the t-value to be large and p-value to be small. P-value in particular is the probability of the 'true' coeefficient being 0 given the observed data. So in this example the estimate is `0.64622` and the probability that it is actually 0 given the the data is 0. In this case the results is said to be 'statistically significant'.








